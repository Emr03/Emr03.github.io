[ { "title": "Perturbed Optimization", "url": "/posts/Perturbed-Optimization/", "categories": "Notes", "tags": "optimization", "date": "2021-12-18 00:00:00 +0000", "snippet": "Many forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable with respect to their parameters either, their smooth approximations can be used to optimize over the underlying parameters.$$\\newcommand\\testmacro[2]{\\mathbf{F\\alpha}(#1)^{#2}}\\def\\norm#1{\\left\\|{#1}\\right\\|} % A norm with 1 argument\\newcommand\\zeronorm[1]{\\norm{#1}_0} % L0 norm\\newcommand\\onenorm[1]{\\norm{#1}_1} % L1 norm\\newcommand\\twonorm[1]{\\norm{#1}_2} % L2 norm\\def\\&amp;lt;{\\left\\langle} % Angle brackets\\def\\&amp;gt;{\\right\\rangle}\\newcommand\\inner[1]{\\langle #1 \\rangle} % inner product\\newcommand\\argmax{\\mathop\\mathrm{arg max}} % Defining math symbols\\newcommand\\argmin{\\mathop\\mathrm{arg min}}\\newcommand{\\pd}[2]{\\frac{\\partial{#1}}{\\partial{#2}}}\\newcommand{\\pdd}[2]{\\frac{\\partial^2{#1}}{\\partial{#2}^2}}$$IntroductionMany forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable either, their smooth approximations can be used to optimize over the underlying parameters.Consider a Linear Program of the form:\\[\\begin{align}F(\\theta) &amp;amp;= \\max _{y \\in \\mathcal{C}}\\langle y, \\theta\\rangle \\\\y^{*}(\\theta) &amp;amp;= \\underset{y \\in \\mathcal{C}}{\\arg \\max }\\langle y, \\theta\\rangle\\end{align}\\]Where \\(C\\) is a convex polytope. The solution to the above LP is almost always uniquely determined by \\(\\theta\\) and corresponds to a vertex of \\(C\\). To see why, consider the following inequality-form LP:\\[\\begin{equation}\\max _{y \\in \\mathcal{C}}\\langle y, \\theta\\rangle,\\ \\text{ such that } Q y \\geq b\\end{equation}\\]for \\(y \\in \\mathbb{R}^n\\). And recall the following KKT condition, where \\(\\mathcal{A}\\) denotes the set of active inequality constraints:\\[\\begin{align}\\theta &amp;amp;= - Q^T \\lambda^* \\\\ \\lambda_i^* &amp;amp;\\neq 0 \\ \\ \\forall i \\in \\mathcal{A}\\end{align}\\]In general, \\(n\\) columns of \\(Q^T\\) are required to construct the n-dimnesional \\(\\theta\\). So \\(\\lambda^*\\) should have \\(n\\) non-zero entries. This means that \\(y^*\\) is the solution of a system of \\(n\\) linear equations \\(\\bar{Q}y = \\bar{b}\\), where \\(\\bar{Q}\\) and \\(\\bar{b}\\) denote a subset of the rows of \\(Q\\) and \\(b\\) respectively. Effectively, \\(y^*\\) is the intersections of \\(n\\) sides of the polytope, corresponding to a vertex point.However, the maximizer \\(y^{*}\\) is not a differentiable function of \\(\\theta\\). This is because, small perturbations in \\(\\theta\\) do not change the active constraints at \\(y^{*}\\). While the entries \\(\\lambda_i\\) might be different, the effective system of \\(n\\) linear equations remains the same, unless a large enough perturbation to \\(\\theta\\) is applied. This means that the Jacobian of \\(y^*\\) with respect to \\(\\theta\\) is zero for almost all \\(\\theta\\), and undefined at the boundaries of normal cones. This is inconvenient when we wish to optimize the parameter \\(\\theta\\) for some objective, as is often the case in non-differentiable learning or inverse problems. Figure 1: The normal fan of the polytope consists of the regions in which the cost parameters result in the same maximizing vertex.Berthet et al. \\cite{} propose to smooth out the above LP. The intuition behind this approach is as follows. Since we want \\(y^{*}\\) to vary smoothly in \\(\\mathcal{C}\\) with respect to \\(\\theta\\), we can consider an alternative solution which is a convex combination of the \\(v\\) vertices of \\(\\mathcal{C}\\),\\[\\begin{equation}y^{*}(\\alpha) = \\sum_{i=1}^{v} \\alpha_i y_i \\text{ where } \\alpha_i \\in [0, 1]\\end{equation}\\]where \\(\\alpha \\in \\mathbb{R}^{v}\\) is a function of \\(\\theta\\) which will be revealed shortly. Intuitively, looking at the above figure, as \\(\\theta\\) is moved clock-wise, we expect \\(y^*(\\alpha)\\) to move closer to \\(y_2\\). In a way, we want all vertices to contribute to the resulting estimate, while keeping the estimate \\(y^{*}(\\alpha)\\) close to the solution. Note that \\(\\alpha\\) effectively defines a probability mass function \\(p_{\\theta}\\) over the possible solutions \\(\\{y_i\\}_{i=1}^{v}\\).The authors propose to achieve a smooth approximation to \\(y^{*}(\\theta)\\) by taking the expectation of \\(y^{*}(\\theta + \\epsilon Z)\\) where \\(\\epsilon\\) controls the size of the added noise, and \\(Z\\) is a random variable sampled from a Gibbs distribution \\(\\mu(Z) \\propto e^{-\\nu(Z)}\\).\\[\\begin{equation}y^{*}_\\epsilon (\\theta) = \\mathbb{E}_z \\mathrm{arg max}_{y \\in C} \\langle y,\\theta + \\epsilon Z \\rangle\\end{equation}\\]\\[\\begin{equation}F_\\epsilon (\\theta) = \\mathbb{E}_z \\mathrm{max}_{y \\in C} \\langle y,\\theta + \\epsilon Z \\rangle\\end{equation}\\]The gradient of \\(F(\\theta) = \\max_{y \\in C}\\langle y, \\theta \\rangle\\) with respect to \\(\\theta\\) is \\(y^*\\), as long as \\(\\theta\\) remains in the normal cone where \\(y^*\\) is optimal. It is undefined on the boundaries of normal cones. There is a similar relationship between \\(\\nabla_\\theta F_{\\epsilon}(\\theta)\\) and \\(y^*_{\\epsilon}(\\theta)\\):\\[\\begin{align}\\nabla_{\\theta}F_{\\epsilon}(\\theta) &amp;amp;= \\nabla_{\\theta} \\mathbb{E}_z \\mathrm{max}_{y \\in C} \\langle y,\\theta + \\epsilon Z \\rangle = \\mathbb{E}_z \\nabla_{\\theta} \\mathrm{max}_{y \\in C} \\langle y,\\theta + \\epsilon Z \\rangle \\\\\\\\&amp;amp;= \\mathbb{E}_z \\nabla_{\\theta} \\langle y^*(\\theta + \\epsilon Z), \\theta \\rangle = \\mathbb{E}_z y^*(\\theta + \\epsilon Z) = y_{\\epsilon}^*(\\theta)\\end{align}\\]This leads to the fact that:\\[\\begin{equation}\\nabla_{\\theta} y_{\\epsilon}^*(\\theta) = \\nabla_{\\theta}^2 F_\\epsilon(\\theta)\\end{equation}\\]Derivatives of Soft MaximizersTo see why the smooth approximations are differentiable with respect to \\(\\theta\\), consider the integral which evaluates \\(F_\\epsilon (\\theta)\\).\\[\\begin{align}F_\\epsilon (\\theta) &amp;amp;= \\int_{Z} \\max_{y \\in C}\\langle y, \\theta + \\epsilon z \\rangle \\mu(z) \\, dz \\\\\\\\\\nabla_{\\theta} F_\\epsilon (\\theta) &amp;amp;= \\int_{Z} \\nabla_{\\theta} \\max_{y \\in C}\\langle y, \\theta + \\epsilon z \\rangle \\mu(z) \\, dz \\\\\\end{align}\\]Since the integral boundaries don’t depend on \\(\\theta\\) all we have to do is differentiate the integrand and integrate the gradient. Since \\(\\max_{y \\in C}\\langle y, \\theta + \\epsilon z \\rangle\\) is not everywhere differentiable, we consider \\(z&#39; = \\theta + \\epsilon z\\) to be a fixed parameter vector, so now \\(z\\) depends on \\(z&#39;\\) and \\(\\theta\\). A simple change of variable results in:\\[\\begin{align} \\nabla_{\\theta} F_\\epsilon (\\theta) &amp;amp;= \\int_{Z&#39;} \\max_{y \\in C}\\langle y, z&#39; \\rangle \\nabla_{\\theta} \\mu\\left(\\frac{z&#39; - \\theta}{\\epsilon}\\right) \\, \\frac{dz&#39;}{\\epsilon} \\\\ \\\\ &amp;amp;= \\int_{Z&#39;} \\max_{y \\in C}\\langle y, z&#39; \\rangle \\mu\\left(\\frac{z&#39; - \\theta}{\\epsilon}\\right) \\nabla_{\\theta} \\nu\\left(\\frac{z&#39; - \\theta}{\\epsilon}\\right) \\, \\frac{dz&#39;}{\\epsilon} \\\\ \\\\ &amp;amp;= \\mathbb{E} \\left[ F(\\theta + \\epsilon Z) \\nabla_{Z} \\nu\\left(Z \\right) / \\epsilon \\right]\\end{align}\\]It follows that:\\[\\begin{align}\\nabla_{\\theta} y_{\\epsilon}^{*}(\\theta) &amp;amp;= \\nabla_{\\theta}^{2} F_{\\epsilon}(\\theta) \\\\\\\\\\nabla_{\\theta} y_{\\epsilon}^{*}(\\theta) &amp;amp;= \\nabla_{\\theta}^{2} \\int_{Z} \\max_{y \\in C}\\langle y, \\theta + \\epsilon z \\rangle \\mu(z) \\, dz \\\\\\\\&amp;amp;= - \\int_{Z&#39;} \\max_{y \\in C}\\langle y, z&#39; \\rangle \\nabla^{2}_{\\theta} \\mu\\left(\\frac{z&#39; - \\theta}{\\epsilon}\\right) \\, \\frac{dz&#39;}{\\epsilon} \\\\\\\\&amp;amp;= - \\int_{Z&#39;} \\max_{y \\in C}\\langle y, z&#39; \\rangle \\nabla_{\\theta} \\left[ \\mu\\left(\\frac{z&#39; - \\theta}{\\epsilon}\\right) \\nabla_{\\theta} \\nu\\left(\\frac{z&#39; - \\theta}{\\epsilon}\\right) \\right] \\, \\frac{dz&#39;}{\\epsilon} \\\\\\\\&amp;amp;= \\mathbf{E} \\left[F(\\theta+\\varepsilon Z)\\left(\\nabla_{z} \\nu(Z) \\nabla_{z} \\nu(Z)^{\\top}-\\nabla_{z}^{2} \\nu(Z)\\right) / \\varepsilon^{2} \\right]\\end{align}\\]Monte-Carlo Estimates of the GradientsUsing the following derived expression:\\[\\begin{equation}\\nabla_{\\theta} y_{\\epsilon}^{*}(\\theta) = \\mathbf{E} \\left[F(\\theta+\\varepsilon Z)\\left(\\nabla_{z} \\nu(Z) \\nabla_{z} \\nu(Z)^{\\top}-\\nabla_{z}^{2} \\nu(Z)\\right) / \\varepsilon^{2} \\right]\\end{equation}\\]we see that we can estimate the derivative of \\(\\nabla_{\\theta} y_{\\epsilon}^{*}(\\theta)\\) by: sampling \\(Z_i \\sim \\mu(Z)\\) for \\(i = 1, ... ,m\\) solving each LP \\(\\max_{y \\in C} \\langle y, \\theta + \\epsilon Z_i \\rangle\\) computing \\(\\left(\\nabla_{z} \\nu(Z) \\nabla_{z} \\nu(Z)^{\\top}-\\nabla_{z}^{2} \\nu(Z)\\right) / \\varepsilon^{2}\\) at \\(Z = Z_i\\).Fenchel-Young Losses (??)First, we write \\(F_\\epsilon (\\theta)\\) with respect to \\(\\theta\\) as a convex conjugate. Let \\(\\Omega(y)\\) be the convex conjugate of \\(F_1 (\\theta)\\). A simple change of variable, where \\(\\theta&#39; := \\epsilon \\theta\\) shows that the convex conjugate of \\(F_{\\epsilon}(\\theta)\\) is \\(\\epsilon \\Omega(y)\\).\\[\\begin{align}\\Omega(y) &amp;amp;= \\sup_{\\theta}\\{\\langle \\theta, y \\rangle - F_1(\\theta)\\} \\\\\\\\\\epsilon \\Omega(y) &amp;amp;= \\sup_{\\theta} \\{\\langle \\epsilon \\theta, y \\rangle - \\mathbb{E}_Z \\mathrm{max}_{y \\in C} \\langle y, \\epsilon \\theta + \\epsilon Z \\rangle\\} \\\\\\\\\\epsilon \\Omega(y) &amp;amp;= \\sup_{\\theta&#39;} \\{\\langle \\theta&#39;, y \\rangle - \\mathbb{E}_Z \\mathrm{max}_{y \\in C} \\langle y, \\theta&#39; + \\epsilon Z \\rangle\\} \\\\\\\\\\epsilon \\Omega(y) &amp;amp;= F^{*}_{\\epsilon}(y)\\end{align}\\]Then using the fact that \\(F_{\\epsilon}(\\theta)\\) is strictly convex:\\[\\begin{align}F_{\\epsilon}(\\theta)= \\sup_{y} \\{\\langle y, \\theta \\rangle - \\epsilon \\Omega(y)\\}\\end{align}\\]The supremum is achieved at \\(y_{\\epsilon}^{*}(\\theta)\\) where \\(\\epsilon \\nabla_{y}\\Omega(y_{\\epsilon}^{*}) = \\theta\\). Using the property \\(\\nabla f^{*} = (\\nabla f)^{-1}\\),\\[\\begin{align}\\left( \\epsilon \\nabla_{y} \\Omega(y_{\\epsilon}^{*}) \\right)^{-1} &amp;amp;= \\nabla_{\\theta} F_{\\epsilon}(\\theta) \\\\\\\\y_{\\epsilon}^{*}(\\theta) &amp;amp;= \\nabla_{\\theta} F_{\\epsilon}(\\theta)\\end{align}\\]Consider the problem of maximizing the likelihood of observations \\(\\{y_i\\}\\), with respect to a parameterized model \\(p_{\\theta}(y)\\), where \\(p_{\\theta}\\) is a Gibbs distribution. The empirical loss is shown below.\\[\\begin{align}\\bar{\\ell}_{n}(\\theta) &amp;amp;= \\frac{1}{n} \\sum_{i=1}^{n} \\log p_{\\text {Gibbs }, \\theta}\\left(y_{i}\\right) \\\\\\\\ &amp;amp;= \\frac{1}{n} \\sum_{i=1}^{n}\\left\\langle y_{i}, \\theta\\right\\rangle-\\log Z(\\theta)\\\\ \\text { with } \\nabla_{\\theta} \\bar{\\ell}_{n}(\\theta) &amp;amp;= \\frac{1}{n} \\sum_{i=1}^{n} y_{i}-\\mathbf{E}_{\\text {Gibbs }, \\theta}[Y]\\end{align}\\]Notice that setting \\(\\nabla_{\\theta} \\bar{\\ell}_{n}(\\theta) = 0\\) points to a moment matching procedure to fit \\(\\theta\\). However,\\[\\begin{align}F_{\\epsilon}(\\theta) &amp;amp;= \\mathbb{E}_Z \\max_{y} \\langle y, \\theta + \\epsilon Z \\rangle \\\\ &amp;amp;\\geq \\mathbb{E}_Z \\log{ \\mathbb{E}_Y exp\\{\\langle Y, \\theta + \\epsilon Z \\rangle\\} } \\\\ &amp;amp;\\geq Z_{\\epsilon}(\\theta) \\\\\\end{align}\\]So \\(\\left\\langle y_{i}, \\theta\\right\\rangle- F_{\\epsilon}(\\theta)\\) is a lower bound on \\(\\ell}_{n}(\\theta)\\).How Good are MC Estimates of the Gradients?To answer this question we need to develop asymptotic results about the MC estimates of \\(\\nabla_{\\theta} y_{\\epsilon}^{*}(\\theta)\\) and \\(\\nabla_{\\theta} F_\\epsilon (\\theta)\\). But before we do, since asymptotic results depend on properties of function of the random variable, we should first characterize \\(F_\\epsilon (\\theta)\\) and \\(y_{\\epsilon}^{*}(\\theta)\\).Properties of \\(F_{\\epsilon}\\): \\(F_{\\epsilon}\\) is strictly convex (with respect to \\(\\theta\\)).Proof:Since \\(F\\) is the supremum of a set of linear functions, it is convex. Let \\(\\lambda \\in \\[0, 1\\]\\), \\(\\theta_{\\lambda}=\\lambda \\theta+(1-\\lambda) \\theta^{\\prime}\\). So:\\[\\begin{equation}\\lambda F_{\\varepsilon}(\\theta)+(1-\\lambda) F_{\\varepsilon}\\left(\\theta^{\\prime}\\right)=\\mathbf{E}\\left[\\lambda F(\\theta+\\varepsilon Z)+(1-\\lambda) F\\left(\\theta^{\\prime}+\\varepsilon Z\\right)\\right] \\geqslant \\mathbf{E}\\left[F\\left(\\lambda \\theta+(1-\\lambda) \\theta^{\\prime}+\\varepsilon Z\\right)\\right]=F_{\\varepsilon}\\left(\\theta_{\\lambda}\\right)\\end{equation}\\]To show that \\(F_{\\epsilon}\\) is strictly convex, we need to show that the inequality above is strict for all values of \\(\\theta, \\ \\theta^{\\prime} \\in \\mathbb{R}^{d}\\). Note that the inequality applies to the arguments inside the expectation. Therefore, for the inequality to hold with equality, it should hold for almost all values of \\(Z\\).If \\(\\lambda F(\\theta+\\varepsilon z)+(1-\\lambda) F\\left(\\theta^{\\prime}+\\varepsilon z\\right) \\geqslant F\\left(\\lambda \\theta+(1-\\lambda) \\theta^{\\prime}+\\varepsilon z\\right)\\), for almost all \\(z\\), then \\(F\\) is linear over the segment \\([\\theta + \\epsilon z, \\theta^{\\prime} + \\epsilon z]\\) for almost all \\(z \\in \\mathbb{R}^{d}\\). Since \\(F\\) is the result of a linear program, \\(F\\) is the value obtained by \\(\\langle \\theta + \\epsilon z, y^{*} \\rangle\\) at \\(y^{*}\\) a vertex point in \\(\\mathcal{C}\\). So if \\(F\\) is linear in \\([\\theta, \\theta^{\\prime}]\\) then \\(y^{*}(\\theta + \\epsilon z)\\) remains the same as \\(\\theta\\) is varied on the line \\([\\theta, \\theta^{\\prime}]\\), for almost all \\(z\\). For this to hold, \\(\\theta + \\epsilon z\\) and \\(\\theta^{\\prime} + \\epsilon z\\) need to be in the same normal cone of \\(\\mathcal{C}\\) for almost all \\(z\\). This can only happen if \\(\\theta - \\theta^{\\prime} = 0\\). So \\(F_{\\epsilon}\\) is strictly convex. \\(F_{\\epsilon}\\) is \\(R_C-Lipschitz\\).Proof:Since \\(F_{\\epsilon}\\) is the supremum of a set of linear functions, and \\(\\twonorm{y} \\leq R_C\\), then \\(| F_{epsilon}(\\theta) - F_{\\epsilon}(\\theta^{\\prime}) | \\leq R_C (\\theta - \\theta^{\\prime})\\). \\(F_{\\epsilon}\\) is \\(R_C M_{\\mu} / \\epsilon\\)-gradient Lipschitz.Proof:Recall that \\(\\nabla_{\\theta} F_\\epsilon (\\theta) = \\mathbb{E} F(\\theta + \\epsilon Z) \\frac{\\nabla_{z}\\mu(z)}{\\epsilon}\\). Then,\\[\\begin{align}\\twonorm{F_\\epsilon (\\theta) - F_\\epsilon (\\theta^{\\prime})} &amp;amp;= \\twonorm{ \\mathbb{E} \\left[ F(\\theta + \\epsilon Z) - F(\\theta^{\\prime} + \\epsilon Z) \\frac{\\nabla_{z}\\mu(z)}{\\epsilon} \\right]}\\\\\\end{align}\\]Using Jensen’s inequality since the $l2$ norm is convex, and the Cauchy-Schwarz inequality we obtain:\\[\\begin{align}&amp;amp; \\leq \\mathbb{E} \\twonorm{ F(\\theta + \\epsilon Z) - F(\\theta^{\\prime})} \\mathbb{E} \\twonorm{\\frac{\\nabla_{z}\\mu(z)}{\\epsilon}} \\\\&amp;amp; \\leq R_C (\\theta - \\theta^{\\prime}) M_{\\mu} / {\\epsilon} \\\\\\end{align}\\]**Asymptotic Convergence of $$\\" }, { "title": "Iterated Soft Thresholding", "url": "/posts/Iterated-Soft-Thresholding/", "categories": "Notes", "tags": "sparsity", "date": "2021-11-08 00:00:00 +0000", "snippet": "$$\\newcommand\\testmacro[2]{\\mathbf{F\\alpha}(#1)^{#2}}\\def\\norm#1{\\left\\|{#1}\\right\\|} % A norm with 1 argument\\newcommand\\zeronorm[1]{\\norm{#1}_0} % L0 norm\\newcommand\\onenorm[1]{\\norm{#1}_1} % L1 norm\\newcommand\\twonorm[1]{\\norm{#1}_2} % L2 norm\\def\\&amp;lt;{\\left\\langle} % Angle brackets\\def\\&amp;gt;{\\right\\rangle}\\newcommand\\inner[1]{\\langle #1 \\rangle} % inner product\\newcommand\\argmax{\\mathop\\mathrm{arg max}} % Defining math symbols\\newcommand\\argmin{\\mathop\\mathrm{arg min}}\\newcommand{\\pd}[2]{\\frac{\\partial{#1}}{\\partial{#2}}}\\newcommand{\\pdd}[2]{\\frac{\\partial^2{#1}}{\\partial{#2}^2}}$$### Introduction$$\\begin{equation}\\min_x \\twonorm{Dx - y}^2 \\quad \\text { subject to } \\zeronorm{x} \\leq k\\end{equation}$$$$\\begin{equation}\\min_x \\frac{1}{2}\\twonorm{Dx - y}^2 \\quad \\text { subject to } \\onenorm{x} \\leq \\tau\\end{equation}$$$$\\begin{equation}L(x, \\lambda) = \\frac{1}{2} \\twonorm{Dx - y}^2 - \\lambda (\\tau - \\onenorm{x})\\end{equation}$$At the optimal solution (which is unique if $$D^TD$$ is full rank):$$\\begin{equation}D^T(Dx - y) + \\lambda sign(x) = 0\\end{equation}$$Proximal Method:objective $$F(x) = f(x) + g(x)$$, with $$f(x)$$ continuously differentiable and $$g(x)$$ not differentiable.Recall Newton&#39;s method for finding roots:x_{k+1} = x_k - (D^T D)^{-1}(D^T(Dx - y) + \\lambda sign(x_k))x_{k+1} = (D^T D)^{-1}D^Ty + \\lambda sign(x_k)$$F(x + p) \\sim f(x) + \\nabla f(x)^T p + \\frac{1}{2}p^T H p + g(x + p)$$Proximal operator with respect to g(x + p)." }, { "title": "Duality I", "url": "/posts/Duality-I/", "categories": "Notes", "tags": "optimization", "date": "2021-11-08 00:00:00 +0000", "snippet": "In this post I present a brief overview of duality in optimization. IntroductionConsider a constrained optimization problem:\\[\\begin{equation}\\min _{x \\in \\mathbb{R}^{n}} f(x) \\quad \\text { subject to }\\left\\{\\begin{array}{ll}c_{i}(x)=0, &amp;amp; i \\in \\mathcal{E} \\\\c_{i}(x) \\geq 0, &amp;amp; i \\in \\mathcal{I},\\end{array}\\right.\\end{equation}\\]In a previous post on the KKT conditions, we looked at the Lagrangian formulation of constrained optimization problems. Specifically, at a local optimizer \\(x^*\\), we have the following necessary conditions:\\[\\begin{equation}\\begin{aligned}\\nabla_{x} \\mathcal{L}\\left(x^{*}, \\lambda^{*}\\right) &amp;amp;=0, &amp;amp; &amp;amp; \\\\c_{i}\\left(x^{*}\\right) &amp;amp;=0, &amp;amp; &amp;amp; \\text { for all } i \\in \\mathcal{E} \\\\c_{i}\\left(x^{*}\\right) &amp;amp; \\geq 0, &amp;amp; &amp;amp; \\text { for all } i \\in \\mathcal{I} \\\\\\lambda_{i}^{*} &amp;amp; \\geq 0, &amp;amp; &amp;amp; \\text { for all } i \\in \\mathcal{I} \\\\\\lambda_{i}^{*} c_{i}\\left(x^{*}\\right) &amp;amp;=0, &amp;amp; &amp;amp; \\text { for all } i \\in \\mathcal{E} \\cup \\mathcal{I}\\end{aligned}\\end{equation}\\]The Dual Problem and Weak DualityConsider \\(L(x, \\lambda) = f(x) - \\sum_i \\lambda_i c_i(x) - \\sum_j \\mu_j c_j(x)\\) for \\(i \\in \\mathcal{I}\\) and \\(j \\in \\mathcal{E}\\). Suppose that we restrict \\(x\\) to the feasible set, and we restrict \\(\\lambda_i \\geq 0\\). It is obvious that \\(L(x, \\lambda, \\mu) \\leq f(x)\\). If in addition, \\((x, \\lambda, \\mu)\\) satisfies the complementarity conditions \\(\\lambda_i c_i(x) = 0\\), then \\(L(x, \\lambda, \\mu) = f(x)\\). In summary:For all feasible \\(x\\) and \\(\\lambda \\geq 0\\):\\begin{equation} L(x, \\lambda, \\mu) \\leq f(x)\\end{equation}With equality if \\(\\lambda_i c_i(x)=0\\) for all \\(i \\in \\mathcal{I}\\).It follows immediately that for all feasible \\(x\\):\\begin{equation} g(\\lambda, \\mu) = \\inf_x L(x, \\lambda, \\mu) \\leq f(x) ,\\ \\forall \\lambda \\leq 0\\end{equation}Naturally, if \\(x^**\\) is a feasible minimizer of \\(f(.)\\):\\begin{equation} g(\\lambda, \\mu) \\leq f(x^*),\\ \\forall \\lambda \\leq 0\\end{equation}This property is known as Weak Duality, and it states that \\(g(\\lambda, \\mu)\\) is a lower bound for \\(f(x)\\) for all feasible \\(x\\), and by extension, it is a lower bound for the solution \\(f(x^*)\\). Note that \\(g(\\lambda, \\mu)\\) can be a vacuous lower bound for some values of \\(\\lambda\\). For example if we have a linear constraint \\(c^T x \\geq 0\\) and a nonzero \\(\\lambda &amp;gt; 0\\), then \\(g(\\lambda, \\mu) = -\\infty\\).Since \\(g(\\lambda, \\mu)\\) is the infimum of a set of affine functions of \\(\\lambda, \\mu\\), it is concave with respect to \\((\\lambda, \\mu)\\). Note that this is true regardless of whether or not \\(f(x)\\) is convex.If we are the optimistic type we might see that there is a possibility for us to solve the original problem by maximizing \\(g(\\lambda, \\mu)\\) over \\(\\lambda \\geq 0\\). To see why, recall that \\(g(\\lambda, \\mu)\\) is a lower bound for our objective \\(f(x)\\) for all feasible \\(x\\), perhaps the largest value of \\(g(\\lambda, \\mu)\\) at \\(\\lambda \\geq 0\\) is equal to the smallest value of \\(f(x)\\) at feasible \\(x\\). Duality theory finds conditions under which this is the case. We refer to the original problem as the primal problem and the following problem as the dual:\\[\\begin{align}\\max_{\\lambda, \\mu} \\inf_x L(x, \\lambda, \\mu) \\\\\\lambda \\geq 0 \\\\\\end{align}\\]Note that the dual problem does not explicitly restrict \\(x\\) to the feasible set.However, the solution \\((x&#39;, \\lambda&#39;, \\mu&#39;)\\) of the dual can never have infeasible \\(x&#39;\\). To see why, suppose that for some \\(\\lambda&#39;, \\mu&#39;\\), \\(x&#39;\\) is the minimizer of \\(f(x) - \\lambda&#39; c_i(x) - \\mu&#39; c_j(x)\\) and it is infeasible.If we assume that \\((x&#39;, \\lambda&#39;, \\mu&#39;)\\) is the solution of the dual, we should have \\(\\lambda&#39;, \\mu&#39;\\) as the maximizers of \\(f(x&#39;) - \\lambda c_i(x&#39;) - \\mu c_j(x&#39;)\\).Case 1: inequality constraint \\(\\mathbf{c_i(x) \\geq 0}\\) is violated:Since \\(c_i(x&#39;) &amp;lt; 0\\), its corresponding multiplier \\(\\lambda\\) increases the outer maximization objective as it gets larger. We have that: \\(f(x&#39;) - (\\lambda + \\Delta) c_i(x&#39;) - \\mu&#39; c_j(x) &amp;gt; f(x&#39;) - \\lambda c_i(x&#39;) - \\mu&#39; c_j(x)\\) for all \\(\\Delta &amp;gt; 0\\). However as \\(\\lambda\\) gets larger, the solution of the inner minimization cannot remain infeasible, since as \\(\\lambda\\) grows we incur a larger penalty from \\(-\\lambda c_i(x)\\). We therefore have a contradiction.Case 2: equality constraint \\(\\mathbf{c_j(x) = 0}\\) is violatedSuppose without loss of generality \\(c_j(x&#39;) &amp;gt; 0\\), its corresponding multiplier \\(\\mu\\) increases the outer maximization objective as its value tends to \\(-\\infty\\). We have that \\(f(x&#39;) - \\lambda&#39; c_i(x&#39;) - (\\mu + \\Delta) c_j(x) &amp;gt; f(x&#39;) - \\lambda&#39; c_i(x&#39;) - \\mu c_j(x)\\) for all \\(\\Delta &amp;lt; 0\\). However as \\(-\\mu\\) gets larger, the solution of the inner minimization cannot remain infeasible, since as \\(-\\mu\\) grows we incur a larger penalty from \\(-\\mu c_i(x)\\). We therefore have a contradiction.We have effectively replaced the constraints on \\(x\\) with a max-min problem.Since the solution of the dual problem cannot be infeasible, this can make it a good alternative in cases where the primal problem is difficult to solve. In the next section we examine conditions under which the dual objective is not just a lower bound, but an equivalent objective to the primal problem.Strong DualityWe know that \\(g(\\lambda, \\mu)\\) is a lower bound for \\(f(x)\\) for all feasible \\(x\\), for all \\(\\lambda \\geq 0\\). And we know that if we maximize over \\(\\lambda, \\mu\\) we can obtain the tightest lower bound. But can we obtain the solution of theprimal problem?We first show that if \\(f(.)\\) and \\(\\{c(.)\\}\\) are convex, the solution of the primal is a solution of the dual. We then show that under stricter conditions, a solution of the dual is a solution of the primal.ReferencesNocedal, Jorge, and Stephen Wright. Numerical Optimization. Springer Science &amp;amp; Business Media, 2006." }, { "title": "Spectral Clustering from the Min Cut Problem", "url": "/posts/Spectral-Clustering/", "categories": "Notes", "tags": "graphs", "date": "2021-10-31 00:00:00 +0000", "snippet": "Spectral clustering, often used as a community detection method, has its origins in the min-cut problem in graph theory. While the min-cut problem is NP-hard in the general case, a relaxation of the problem can be obtained from the graph Laplacian.$$\\newcommand\\testmacro[2]{\\mathbf{F\\alpha}(#1)^{#2}}\\def\\norm#1{\\left\\|{#1}\\right\\|} % A norm with 1 argument\\newcommand\\zeronorm[1]{\\norm{#1}_0} % L0 norm\\newcommand\\onenorm[1]{\\norm{#1}_1} % L1 norm\\newcommand\\twonorm[1]{\\norm{#1}_2} % L2 norm\\def\\&amp;lt;{\\left\\langle} % Angle brackets\\def\\&amp;gt;{\\right\\rangle}\\newcommand\\inner[1]{\\langle #1 \\rangle} % inner product\\newcommand\\argmax{\\mathop\\mathrm{arg max}} % Defining math symbols\\newcommand\\argmin{\\mathop\\mathrm{arg min}}\\newcommand{\\pd}[2]{\\frac{\\partial{#1}}{\\partial{#2}}}\\newcommand{\\pdd}[2]{\\frac{\\partial^2{#1}}{\\partial{#2}^2}}$$Spectral clustering, often used as a community detection method, has its origins in the min-cut problem in graph theory. While the min-cut problem is NP-hard in the general case, a relaxation of the problem can be obtained from the graph Laplacian.We will start by introducing the objective for the min-cut problem which intuitively captures some notion of clusters or communities on graphs. Since this objective is NP-hard in the general case, we look at the derivation of an alternate objective based on an ideal case where the graph consists of $k$ connected components. In the following discussion we describe a cut $S$ as the set of edges that connect nodes in different partitions.Graph Cut Based on ConductanceThe conductance objective for a cut S is as follows:\\[\\begin{equation}\\phi(A_1, A_2, ... A_k) = \\sum_{l=1}^{k}\\frac{\\sum_{(i, j) \\in A_l \\times \\bar{A}_l} w_{ij}}{vol(A_l)}\\end{equation}\\]where \\(vol(A_l)\\) is the total weighted degree (sum of weights of edges connected to nodes in \\(A_l\\)) of the nodes in \\(A_l\\).Note that for a fixed cut cost \\(\\sum_{(i, j) \\in S} w_{ij}\\), the conductance between the two partitions is higher if either $vol(A)$ or \\(vol(B)\\) is too small. That is, we want to keep a substantial edge mass in both partitions.Intuitively, computing the optimal cut in the general case is NP-hard, since there are exponentially many candidate graph cuts for a graph with \\(N\\) edges. One can compare candidate solutions in polynomial time but there is no easy way to compute an improvement since the problem does not have a greedy or recursive sub-problem structure.Spectral Graph PartitioningIn the special case where the graph already consists of 2 connected, d-regular components, the optimal conductance is $0$, and we will show how to find the partitions by looking at the graph’s adjacency matrix and graph Laplacian. We will then compare other cases to this ideal case and derive an approximate solution to the minimum conductance criterion.The Eigenvectors of the Ideal Case:For a d-regular graph, the most obvious eigenvector of the adjacency matrix \\(A\\) is the all ones vector \\(\\mathbb{1}\\), with eigenvalue $d$ which is the largest eigenvalue of \\(A\\).For a graph with k disjoint d-regular components, the largest eigenvalue is still \\(d\\), however this eigenvalue now has multiplicity \\(k\\). The eigenspace of \\(d\\) is spanned by eigenvectors consisting of all 1’s at the indices corresponding to nodes in a single connected component. In this ideal case, if we know the eigenvectors corresponding to the largest eigenvalue of the adjacency matrix, we can obtain the \\(k\\) connected components corresponding to the min-cut partitions.What if the graph is connected?What if we were to move a few edges to connect the separate partitions, while maintaining d-regularity? Now our graph is connected, and its largest eigenvalue is still \\(d\\) corresponding to the eigenvector of all ones.Now the eigenvalue \\(d\\) has multiplicity \\(1\\) but intuitively we can partition or permute the rows of \\(A\\) into an almost block-diagonal matrix. Since by construction the off-block diagonal entries are limited compared to \\(d\\) (remember conductance?), the second largest eigenvalue is close to $d$.Also, since eigenvectors are orthogonal, the entries of the second largest eigenvector must sum to 0. So we have entries that are non-negative and entries that are negative. The sign of the entries can be used to determine the partition of the graph into two components.What if we require more than 2 partitions?Understanding the k-partition problem requires us to understand the graph Laplacian \\(L = D - W\\) where \\(D\\) is the weighted degree matrix and \\(W\\) is the adjacency matrix.Note that:\\[\\begin{align}x^T L x &amp;amp;= x^T(D - W)x \\\\&amp;amp;= \\sum_{i}d_i x_i^2 - 2\\sum_{(i, j)\\ in E} w_{ij} x_i x_j \\\\&amp;amp; = \\sum_{(i, j)\\ in E}\\left( \\sum_j w_{ij} x_i^2 + \\sum_i w_{ji} x_j^2 - 2 w_{ij} x_i x_j \\right)\\\\&amp;amp; = \\sum_{(i, j)\\ in E} w_{ij}(x_i - x_j)^2\\end{align}\\]We can therefore think of the Laplacian as an operator on \\(x \\in \\mathbb{R}^n\\) which is a vector of values assigned to nodes. If adjacent nodes are assigned similar values, then \\(x^T L x\\) takes on a small value. Note that the all ones vector \\(\\mathbb{1}\\) is an eigenvector of the Laplacian matrix, with eigenvalue \\(0\\).Let \\(cut(A_l, \\bar{A}_l) := \\sum_{(i, j) \\in A_l \\times \\bar{A}_l} w_{ij}\\).Note the similarity between the expressions for \\(cut(A_l, \\bar{A}_l)\\) and \\(x^T L x\\).In fact we can write \\(cut(A_l, \\bar{A}_l)\\) in terms of the Laplacian matrix, if we know how to define \\(x\\).Specifically if we set \\(x\\) such that \\(x_i = x_j\\) for any \\(i, j\\) in \\(A_l\\) or any \\(i, j\\) in \\(\\bar{A}_l\\), then \\(x^T L x\\) reduces to \\(\\sum_{(i, j) \\in A_l \\times \\bar{A}_l} w_{ij} (x_i - x_j)^2\\).If we also set\\(x_i = \\frac{1}{\\sqrt{vol(A_l)}}\\) if \\(i \\in A_l\\), and \\(x_i = 0\\) otherwise, we obtain\\[\\begin{equation}N \\cdot vol(A) \\cdot cut(A_l, \\bar{A}_l) = x^T L x\\end{equation}\\]The vector \\(x\\) acts like an indicator vector for partition \\(A_l\\), we can define such vectors \\(x_l\\) for each partition obtaining a set of \\(k\\) vectors in \\(\\mathbb{R}^n\\), \\(\\{x_l\\}_{l=1}^{k}\\).We obtain a different formulation for the min-cut problem:\\[\\begin{equation}min_{(A_1, ... A_l)} \\sum_{l=1}^{k} cut(A_l, \\bar{A}_l) = min_{(A_1, ... A_l)} \\sum_{l=1}^{k} x_l^T L x_l\\end{equation}\\]Let \\(H \\in \\mathbb{R}^{n \\times k}\\) be a matrix whose columns are \\(\\{x_l\\}_{l=1}^{k}\\). Then we can write:\\[\\begin{equation} \\sum_{l=1}^{k} x_l^T L x_l = Tr(H^T L H)\\end{equation}\\]Note that we still have a discrete constraint on \\(H\\), where \\(H_{ij} = \\frac{1}{\\sqrt{vol(A_j)}}\\) or \\(H_{ij} = 0\\). Also \\(vol(A_j)\\) depends on \\(x_j\\).We can relax this constraint by allowing the entries of \\(H\\) to take any real values, as long as \\(H^TH = I\\). The resulting relaxed objective for the min-cut problem becomes:\\[\\begin{equation}min_{H: H^T D H = I} Tr(H^T L H)\\end{equation}\\]Let \\(T = D^{\\frac{1}{2}}H\\)\\[\\begin{equation}min_{H: T^T T = I} Tr(T^T D^{-1\\frac{1}{2}} L D^{-1\\frac{1}{2}} T)\\end{equation}\\]Using the Rayleigh quotient, we can show that the solution to the above problem consists of the top \\(k\\) eigenvectors of \\(D^{-1\\frac{1}{2}} L D^{=1\\frac{1}{2}}\\) which is the normalized Laplacian matrix. But the eigenvectors do not have the discrete structure corresponding to partitions which we defined above. Here is where we rely on heuristics.Since \\(T = D^{\\frac{1}{2}}H\\), \\(T\\) is obtained from \\(H\\) by re-weighting the rows of \\(H\\). Recall that the rows of \\(H\\) (and in turn those of \\(T\\)) are by design expected to indicate the membership of each node. Once the rows of \\(T\\) are normalized, we can cluster them using \\(k\\)-means and obtain a partition.ReferencesVon Luxburg, Ulrike. “A Tutorial on Spectral Clustering.” Statistics and Computing, vol. 17, no. 4, Springer, 2007, pp. 395–416." }, { "title": "How Adversarial Attacks Exploit Shortcuts", "url": "/posts/How-Adversarial-Attacks-Exploit-Shortcuts/", "categories": "Research", "tags": "adversarial robustness", "date": "2021-08-16 00:00:00 +0000", "snippet": "Recent investigations into the nature of adversarial attacks have emphasized the presence of non-robust features in thedataset. Understanding the nature of non-robust features has proven difficult since it requires understanding the set of uninterpretable pixel patterns which neural networks learn to associate with the class label. However, despite this difficulty, a deeper insight into the nature of non-robust features is valuable since it may shed light on the inductive biases of current training methods and thus enable us to develop more robust training algorithms.While I support the notion that adversarial attacks are not random and that they are dependent on dataset structure, I argue for abandoning the notion that non-robust features are present in the dataset and instead walk through an example of how neural networks learn to summarize the data in non-robust ways. In the discussion that follows I allude to common mis-interpretations of the non-robust features hypothesis and propose alternative viewpoints supported by experiments.IntroductionTo make this discussion more concrete, I examine the phenomenon of generalizable adversarial data using a simple synthetic datasets whose generative process can be controlled and understood. This allows us to characterize adversarial attacks in terms of the dataset’s structure without being biased by our own notions of robust features. We first reproduce the observations of Ilyas et al. \\cite{} using the synthetic dataset. We then distill a classifier trained on the dataset into a more interpretable form which allows us to understand the shortcuts learned by the network. We then use the distilled form of the model to construct adversarial attacks.ExperimentsOur \\(31 \\times 31\\) images consist of superpositions of 4 Gabor wavelets out of a feature bank of 56.We refer to a combination of 4 wavelets as a configuration. There are 10 possible class labels, each configuration belongs to exactly one class. The image below shows the feature bank. Linear combinations of 4 features result in samples similar to those shown below: Each resulting image \\(Y\\) admits a sparse representation \\(X\\) with which the image may be constructed as \\(Y = DX\\). Where the columns of \\(D\\) are the features in the feature bank. The sparse representations determine the class label and are linearly separable.Note that this does not mean that the images \\({Y}\\) are linearly separable, since obtaining \\(X\\) from \\(Y\\) requires a non-linear procedure.We train a two-layer MLP on 10000 samples of the dataset.Motivating ExampleWe first illustrate an example of shortcut learning which will motivate the remainder of this work. Suppose that the input samples of the dataset consist of superpositions of at most \\(k\\) out of a possible \\(M\\) features. Consider two features of interest \\(\\mathbf{q}\\) and \\(\\mathbf{z}\\). We want to detect whether feature \\(\\mathbf{q}\\) or feature \\(\\mathbf{z}\\) is present in the input. One parameterization with a scalar output is the following:\\[\\begin{align}y = \\mathbf{u}^T ReLU(W^T \\mathbf{x} - \\mathbf{b}) \\\\u = [1, 1] \\\\W = \\begin{bmatrix} q^T \\\\ z^T\\end{bmatrix}\\end{align}\\]Where \\(\\mathbf{b}\\) is at least the largest possible value of \\(W^Tx\\) for an input \\(\\mathbf{x}\\) that does not contain \\(\\mathbf{q}\\) or \\(\\mathbf{z}\\).Consider the alternate parameterization, with a single weight vector \\(\\mathbf{w}\\):\\[\\begin{align}y = ReLU(\\mathbf{w}^T \\mathbf{x} - \\mathbf{b}) \\\\\\end{align}\\]For \\(y\\) to be \\(1\\) when either \\(\\mathbf{q}\\) or \\(\\mathbf{z}\\) is present in \\(x\\), all that is required of \\(\\mathbf{w}\\) is that it satisfy the following inequality:\\[\\begin{equation}\\begin{bmatrix} q^T \\\\ z^T\\end{bmatrix} \\mathbf{w} \\geq \\begin{bmatrix} b \\\\ b \\end{bmatrix}\\end{equation}\\]While there are infinitely many possible solutions, they all share the property that they have a large enough component in the span of \\(\\mathbf{q}\\) and \\(\\mathbf{z}\\).The figure below illustrates the kind of solutions that can be obtained when features \\(\\mathbf{q}\\) and \\(\\mathbf{z}\\) are not very strongly aligned. Geometrically, \\(\\mathbf{w}\\) is the normal vector of a hyperplane at a distance \\(b\\) from the origin. The weight vector \\(\\mathbf{w}\\) is sufficient to detect the presence of either \\(\\mathbf{q}\\) or \\(\\mathbf{z}\\) but is not strongly aligned with either of them. The vector \\(\\boldsymbol{\\delta}\\) of much lower magnitude is better aligned with \\(\\mathbf{w}\\). In this work we demonstrate forms of shortcut learning as shown above, and show how these shortcuts depend on dataset structure. Moreover, we provide evidence that adversarial examples exploit these shortcuts.The image below shows the alignment between the first layer features of the classifier (row indexed) and the dictionary atoms (column indexed). Each classifier feature is weakly aligned with multiple dictionary atoms. As seen below the first layer features are less parsimonious than the dictionary atoms shown above. We can probe further to understand why the above features are effective for classification. Our first step is to distill the model into a more interpretable form. For each sample in the training set, we decompose its configuration of 4 atomic features into a set of unigrams, bigrams, trigrams and 4-grams, where an n-gram is an order-agnostic combination of atomic features. Since the data is sampled from a limited set of configurations, the number of n-grams that decompose these configurations is manageable and is a lot smaller than the total number of possible n-grams. We initialize a counter for each (hidden-unit, n-gram) pair that keeps track of how often the hidden unit was activated when the n-gram was present in the input. We pass the samples through the classfier, and record how often each feature is activated when an n-gram is present in the input. Once we have passed every sample in the training set, we decompose each hidden unit into a logical expression of the form \\(Out_j = \\or (g_i)_{g_i \\in S_j}\\) where \\(g_i\\) denotes an n-gram, and \\(S_j\\) denotes the set of n-grams that consistently activate unit \\(j\\). We can thus obtain a logical expression which is consistent with the neural network classifier and explains its prediction for a given sample from the dataset as follows:" }, { "title": "Constrained Optimization, The KKT conditions", "url": "/posts/Constrained-Optimization-the-KKT-conditions/", "categories": "Notes", "tags": "optimization", "date": "2021-07-24 00:00:00 +0000", "snippet": "In this post I present an intuitive description of constrained optimization. It is by no means self-contained. A bit of background knowledge in constrained optimization is assumed. This post is mostly useful for those who would like a more concise and intuitive take of some core ideas, or a brief introduction to Chapter 12 of Numerical Optimization by Nocedal and Wright.$$\\newcommand\\testmacro[2]{\\mathbf{F\\alpha}(#1)^{#2}}\\def\\norm#1{\\left\\|{#1}\\right\\|} % A norm with 1 argument\\newcommand\\zeronorm[1]{\\norm{#1}_0} % L0 norm\\newcommand\\onenorm[1]{\\norm{#1}_1} % L1 norm\\newcommand\\twonorm[1]{\\norm{#1}_2} % L2 norm\\def\\&amp;lt;{\\left\\langle} % Angle brackets\\def\\&amp;gt;{\\right\\rangle}\\newcommand\\inner[1]{\\langle #1 \\rangle} % inner product\\newcommand\\argmax{\\mathop\\mathrm{arg max}} % Defining math symbols\\newcommand\\argmin{\\mathop\\mathrm{arg min}}\\newcommand{\\pd}[2]{\\frac{\\partial{#1}}{\\partial{#2}}}\\newcommand{\\pdd}[2]{\\frac{\\partial^2{#1}}{\\partial{#2}^2}}$$In this post I present an intuitive descriptionof constrained optimization. It is by no means self-contained. A bit of background knowledge in constrained optimization is assumed. This post is mostly useful for those who would like a more concise and intuitive take of some core ideas. This post is best used as a roadmap to Chapter 12 of Numerical Optimization by Nocedal and Wright.Basically we want to solve problems of the form:\\[\\begin{equation}\\min _{x \\in \\mathbb{R}^{n}} f(x) \\quad \\text { subject to }\\left\\{\\begin{array}{ll}c_{i}(x)=0, &amp;amp; i \\in \\mathcal{E} \\\\c_{i}(x) \\geq 0, &amp;amp; i \\in \\mathcal{I},\\end{array}\\right.\\end{equation}\\]And we will make our lives easier by assuming that \\(f(.)\\) and \\(\\{c_i(.)\\}\\) are smooth. This means we can have nice linear approximations of \\(f(.)\\) and \\(\\{c_i(.)\\}\\) around points of interest. We will use \\(\\Omega\\) to denote the feasible set.Before we begin thinking about global optimizers and optimization procedures, we will ask ourselves a seemingly underwhelming question:How do we know that a point \\(x\\) is definitely NOT a local optimizer?The Easy StuffYou’ve probably seen the two canonical examples, one with an equality constraint and one with an inequality constraint.So to save everyone some time I won’t go over them in detail. Feel free to skip ahead if you’re feeling like a smartypants. The main ideas are as follows. When we have an equality constraint, legal directions lie on the surface parameterized by \\(c(.)\\) and are therefore orthogonal to \\(\\nabla_x c(x)\\). If all directions orthogonal to \\(\\nabla_x c(x)\\) are not descent directions then we can’t move from a candidate minimizer \\(x\\) to a better one. This happens when we can’t find a direction \\(d\\) that satisfies: \\(\\begin{align*}\\nabla_x f(x)^T d &amp;amp;&amp;lt; 0 \\\\\\nabla_x c(x)^T d &amp;amp;= 0\\end{align*}\\)or equivalently \\(\\nabla_x f(x) = \\lambda \\nabla_x c(x)^T\\) for some \\(\\lambda \\neq 0\\). When we have an active inequality constraint, legal directions either lie on the surface parameterized by \\(c(.)\\) or along directions of increase of \\(c(.)\\). If all directions at an acute angle from \\(\\nabla_x c(x)\\) are not descent directions then we can’t move from a candidate minimizer \\(x\\) to a better one. This happens when we can’t find a direction \\(d\\) that satisfies: \\(\\begin{align*} \\nabla_x f(x)^T d &amp;amp;&amp;lt; 0 \\\\ \\nabla_x c(x)^T d &amp;amp;\\geq 0 \\end{align*}\\)or equivalently \\(\\nabla_x f(x) = \\lambda \\nabla_x c(x)^T\\) for some \\(\\lambda &amp;gt; 0\\). When we only have one constraint it is easy to find conditions under which there are no feasible directions. But what ifwe have more than one constraint? We basically have a set of linear inequalities in high dimensions and we want to know if the set of solutions is empty. In these cases our geometric intuitions fail us.Also, since \\(c(x)\\) is smooth we can rely on \\(\\nabla_x c(x)\\) to point us to feasible directions. But this may not be thecase when we have multiple competing constraints that reduce the feasible set to a single point, then no direction is a feasible direction. In this case the gradients of the constraint functions are deceiving.This is where the following distinction comes in handy: The tangent cone (to be trusted): from which directions can different paths within the feasible set reach the optimizer? \\(T_{\\Omega}\\left(x^{*}\\right)\\) is the set of all directions \\(d\\) such that, for some sequence of feasible points \\(z_k\\) and vanishing scalars \\(t_k\\):\\[d = \\lim _{k \\rightarrow \\infty} \\frac{z_{k}-x}{t_{k}}\\] Set of linearized feasible directions (can be deceiving): directions that appear legal under a first order Taylor approximation of the constraints.\\[\\mathcal{F}(x)=\\left\\{\\begin{array}{ll} \\left.d \\mid \\begin{array}{ll}d^{T} \\nabla c_{i}(x)=0, &amp;amp; &amp;amp; \\text { for all } i \\in \\mathcal{E}, \\\\ d^{T} \\nabla c_{i}(x) \\geq 0, &amp;amp; &amp;amp; \\text { for all } i \\in \\mathcal{A}(x) \\cap \\mathcal{I}\\end{array}\\right\\}\\end{array}\\right\\}\\]What we are actually interested in is the former, but we can much more easily work with the latter. We should therefore find conditions that guarantee that the tangent cone is equivalent to the set of linearized feasible directions.In Constraint Qualifications We TrustWhen is \\(\\mathcal{F}(x)\\) equivalent to \\(T_{\\Omega}(x)\\)?To show that \\(\\mathcal{F}(x) = T_{\\Omega}(x)\\) under some condition, we need to show that everything in \\(T_{\\Omega}(x)\\) is also in \\(\\mathcal{F}(x)\\) and vice versa.The first part is easy, if \\(d\\) \\(\\in T_{\\Omega}(x)\\) then by definition \\(d\\) is the limiting direction of \\(x - z_k\\) for some sequence of \\(\\{z_k\\}\\) in \\(\\Omega\\). Which means that we can take a sufficiently small step in the direction of \\(d\\) and end up somewhere in the feasible set \\(\\Omega\\). Hence \\(d \\in \\mathcal{F}(x)\\).The second part is not so straightforward. We need to show that under the right conditions, any direction \\(d\\) in \\(\\mathcal{F}(x)\\) is also in \\(T_{\\Omega}(x)\\). More concretely, we need to find a map between each \\(d \\in \\mathcal{F}(x)\\) and some path \\(\\{z_k\\}_{k=0}^{\\infty}\\) in \\(\\Omega\\) whose limiting tangent is \\(d\\).For some fixed \\(d\\), here are the equations that need to be satisfied for some \\(t, z\\):\\[\\begin{align}z = x + t dc_i(z) = c_i(x) + t d^T \\nabla c_i(x)\\end{align}\\]The first equation just expresses the condition for \\(d\\) to be in \\(T_{\\Omega}(x)\\).The second equation depends on the fact that \\(d \\in \\mathcal{F}(x)\\) and therefore if \\(t &amp;gt; 0\\), \\(t d^T \\nabla c_i(x) \\geq 0\\) for active inequality constraints and \\(t d^T \\nabla c_i(x) = 0\\) for equality constraints, making \\(z\\) a feasible point.What we have so far is a system of non-linear equations (because of \\(c_i(z))\\). Which means it’s a good time to use the Implicit Function Theorem (IFT) which may have been gathering dust somewhere in the back of your mind. Here’s a breakdown of how it’s used in our case:The IFT states that for some multivariable function \\(F(z, t)\\), we can find solutions \\((z, t)\\) to the system \\(F(z, t) = 0\\) that are “close enough” to a known solution \\(z=x\\), \\(t=0\\), if the Jacobian \\(\\nabla_z F(z, 0)\\) is invertible. Specifically, the solutions take the form of a function \\(f(t) \\rightarrow z\\).A quick and intuitive interpretation can be provided via a first order Taylor approximation.\\[\\begin{align}0 = F(z, t) = \\nabla_z F(x, 0)(z - x) + \\nabla_t F(x, 0) t \\\\\\nabla_z F(x, 0)(z - x) = - \\nabla_t F(x, 0)t \\\\z = x - \\nabla_z F(x, 0)^{-1} \\nabla_t F(x, 0)t \\\\\\end{align}\\]Since the IFT requires that the Jacobian of \\(F(z, t)\\) with respect to \\(z\\) be invertible, and this is not the case for our current parameterization, we’re going to cheat and construct a different system with an invertible Jacobian. First we’ll assume that the \\(\\{\\nabla c_i(x)\\}_{i \\in \\mathcal{A}}\\) are invertible, and make them rows of the matrix \\(A(x)\\). Then we’ll use a matrix \\(Z\\) instead of \\(I\\) in our first equation, whose columns span the null space of \\(A(x)\\). Now we have a system:\\[\\begin{equation} F(z, t)=\\left[\\begin{array}{c}c(z)-t A\\left(x \\right) d \\\\ Z^{T}\\left(z-x -t d\\right)\\end{array}\\right]=\\left[\\begin{array}{c}0 \\\\ 0\\end{array}\\right]\\end{equation}\\]This makes our Jacobian with respect to \\(z\\) invertible, and the IFT says that for some fixed \\(d\\) given a “small enough” value \\(t\\) we can get a \\(z\\) that satisfies the above conditions.Even though we changed the original system of equations, we know that the solutions of the latter are also solutions of the former. To see this, notice that for small \\(t\\) the first equation can be approximated (using Taylor) by: \\(A(x)(z - x - td)\\), which means that \\((z - x - td)\\) is required to be in the null space of \\([A(x), Z^T]\\), which by construction is \\(\\{0\\}\\). Therefore \\((z - x - td)\\) must be \\(0\\) as specified in the first system.Note that our cheating revealed a nice condition that we can use as a constraint qualification: \\(\\{\\nabla c_i(x)\\}\\) are linearly independent. Note that this condition was used ad hoc to make our system satisfy the conditions of the IFT. It is likely that this sufficient condition is pessimistic, meaning it misses many cases where \\(T_\\Omega(x) = \\mathcal{F}(x)\\).More Than One Constraint and Farkas’ Lemma:Assuming that some constraint qualifications are satisfied, how do we know that there are no feasible directions \\(d\\) from a candidate \\(x\\)?To appreciate the difficulty of finding such a necessary condition when there are multiple constraints, consider all the inequalities that need to be satisfied for all active constraints:\\[\\begin{align}\\nabla f(x)^T d &amp;amp;&amp;lt; 0 \\\\\\nabla c_i(x)^T d &amp;amp; \\geq 0 \\text{ for i } \\in \\mathcal{I} \\cap \\mathcal{A} \\\\\\nabla c_i(x)^T d &amp;amp; = 0 \\text{ for i } \\in \\mathcal{E} \\\\\\end{align}\\]In the case of say two equality constraints and no inequality constraints, there’s no solution if for some \\(\\lambda_1, \\lambda_2 \\neq 0\\):\\[\\begin{equation}\\nabla f(x) = \\lambda_1 \\nabla c_1(x) + \\lambda_2 \\nabla c_2(x) \\\\\\end{equation}\\]Add in an inequality constraint \\(c_3(x)\\). It could be that \\(\\nabla f(x) \\notin span\\{\\nabla c_1(x) , \\nabla c_2(x)\\}\\), yet \\(\\nabla f(x) = \\lambda_3 \\nabla c_3(x)\\) for some \\(\\lambda_3 &amp;gt; 0\\).Note that the set of feasible directions at \\(x\\) is a cone of the form:\\[\\begin{equation}K = \\{B y + C w: y \\geq 0\\}\\end{equation}\\]Where \\(B\\) is a matrix whose rows are \\(\\{\\nabla c_i(x)\\}\\) for \\(i \\in \\mathcal{I} \\cap \\mathcal{A}\\). And \\(C\\) is a matrix whose columns span the subspace orthogonal to \\(\\{\\nabla c_i(x)\\}\\) for \\(i \\in \\mathcal{E}\\). We can now say that \\(d\\) is a feasible descent direction if \\(d \\in K\\) and \\(d^T \\nabla f(x) &amp;lt; 0\\). What remains is obtaining conditions under which the set of feasible descent directions is empty. This will require Farkas’ Lemma.Farkas’ Lemma:For any \\(d\\) \\in \\(\\mathbb{R}^{n}\\), exactly one of the following is true: either \\(d \\in K\\) \\(\\exists\\) \\(g \\in \\mathbb{R}^{n}\\) such that \\(g^T d \\leq 0\\), \\(B^Tg &amp;gt; 0\\), \\(C^Tg = 0\\).To derive conditions under which the set of feasible descent directions is empty, weneed to find conditions under which all vectors \\(d\\) such that \\(d^T \\nabla f(x) &amp;lt; 0\\) are not in \\(K\\). Using Farkas’ Lemma, if \\(B^T \\nabla f(x) \\geq 0\\) and \\(C^T \\nabla f(x) = 0\\), then \\(d\\) cannot be in \\(K\\). Notice how these conditions correspond to the KKT condition.\\[\\nabla f(x) = \\sum_{i \\in \\mathcal{I} \\cap \\mathcal{A}} \\lambda_i \\nabla c_i(x) + \\sum_{i \\in \\mathcal{E}} \\lambda_i \\nabla c_i(x)\\] \\(\\nabla f(x)^T \\nabla c_i(x) \\geq 0\\) for \\(i \\in \\mathcal{I} \\cap \\mathcal{A}\\) \\(\\rightarrow \\lambda_i \\geq 0\\) for \\(i \\in \\mathcal{I} \\cap \\mathcal{A}\\)ReferencesNocedal, Jorge, and Stephen Wright. Numerical Optimization. Springer Science &amp;amp; Business Media, 2006." } ]
