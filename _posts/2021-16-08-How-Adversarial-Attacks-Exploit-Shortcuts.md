---
title: How Adversarial Attacks Exploit Shortcuts
author: Elsa Riachi
categories: [Research]
tags: [adversarial robustness]
date: 2021-08-16
math: true
---


Recent investigations into the nature of adversarial attacks have emphasized the presence of non-robust features in the
dataset. Understanding the nature of non-robust features has proven difficult since it requires understanding the set of uninterpretable pixel patterns which neural networks learn to associate with the class label. However, despite this difficulty, a deeper insight into the nature of non-robust features is valuable since it may shed light on the inductive biases of current training methods and thus enable us to develop more robust training algorithms.


While I support the notion that adversarial attacks are not random and that they are dependent on dataset structure, I argue for abandoning the notion that non-robust features are present in the dataset and instead walk through an example of **how** neural networks learn to summarize the data in non-robust ways. In the discussion that follows I allude to common mis-interpretations of the non-robust features hypothesis and propose alternative viewpoints supported by experiments.

## Introduction

To make this discussion more concrete, I examine the phenomenon of generalizable adversarial data using a simple synthetic datasets whose generative process can be controlled and understood. This allows us to characterize adversarial attacks in terms of the datasetâ€™s structure without being biased by our own notions of robust features. We first reproduce the observations of Ilyas et al. \cite{} using the synthetic dataset. We then distill a classifier trained on the dataset into a more interpretable form which allows us to understand the shortcuts learned by the network. We then use the distilled form of the model to construct adversarial attacks.

## Experiments
Our $$31 \times 31$$ images consist of superpositions of 4 Gabor wavelets out of a feature bank of 56.
We refer to a combination of 4 wavelets as a *configuration*. There are 10 possible class labels, each configuration belongs to exactly one class. The image below shows the feature bank.

<figure>
    <img src="/assets/img/shortcut/images/gabor_filters.png" width="650">
    <figcaption></figcaption>
</figure>

Linear combinations of 4 features result in samples similar to those shown below:

<figure>
    <img src="/assets/img/shortcut/images/samples.png" width="650">
    <figcaption></figcaption>
</figure>

Each resulting image $$Y$$ admits a sparse representation $$X$$ with which the image may be constructed as $$Y = DX$$. Where the columns of $$D$$ are the features in the feature bank. The sparse representations determine the class label and are linearly separable.
Note that this does not mean that the images $${Y}$$ are linearly separable, since obtaining $$X$$ from $$Y$$ requires a non-linear procedure.

We train a two-layer MLP on 10000 samples of the dataset.

## Motivating Example

We first illustrate an example of shortcut learning which will motivate the remainder of this work. Suppose that the input samples of the dataset consist of superpositions of at most $$k$$ out of a possible $$M$$ features. Consider two features of interest $$\mathbf{q}$$ and $$\mathbf{z}$$. We want to detect whether feature $$\mathbf{q}$$ or feature $$\mathbf{z}$$ is present in the input. One parameterization with a scalar output is the following:

$$
\begin{align}
y = \mathbf{u}^T ReLU(W^T \mathbf{x} - \mathbf{b}) \\
u = [1, 1] \\
W = \begin{bmatrix}
 q^T \\
 z^T
\end{bmatrix}
\end{align}
$$

Where $$\mathbf{b}$$ is at least the largest possible value of $$W^Tx$$ for an input $$\mathbf{x}$$ that does not contain $$\mathbf{q}$$ or $$\mathbf{z}$$.

Consider the alternate parameterization, with a single weight vector $$\mathbf{w}$$:

$$
\begin{align}
y = ReLU(\mathbf{w}^T \mathbf{x} - \mathbf{b}) \\
\end{align}
$$

For $$y$$ to be $$1$$ when either $$\mathbf{q}$$ or $$\mathbf{z}$$ is present in $$x$$, all that is required of $$\mathbf{w}$$ is that it satisfy the following inequality:

$$
\begin{equation}
\begin{bmatrix}
 q^T \\
 z^T
\end{bmatrix} \mathbf{w} \geq \begin{bmatrix}
                              b \\
                              b
                              \end{bmatrix}
\end{equation}
$$

While there are infinitely many possible solutions, they all share the property that they have a large enough component in the span of $$\mathbf{q}$$ and $$\mathbf{z}$$.

The figure below illustrates the kind of solutions that can be obtained when features $$\mathbf{q}$$ and $$\mathbf{z}$$ are not very strongly aligned. Geometrically, $$\mathbf{w}$$ is the normal vector of a hyperplane at a distance $$b$$ from the origin. The weight vector $$\mathbf{w}$$ is sufficient to detect the presence of either $$\mathbf{q}$$ or $$\mathbf{z}$$ but is not strongly aligned with either of them. The vector $$\boldsymbol{\delta}$$ of much lower magnitude is better aligned with $$\mathbf{w}$$.

<figure>
    <img src="/assets/img/shortcut/images/shortcut.svg" width="500">
    <figcaption></figcaption>
</figure>

In this work we demonstrate forms of shortcut learning as shown above, and show how these shortcuts depend on dataset structure. Moreover, we provide evidence that adversarial examples exploit these shortcuts.

The image below shows the alignment between the first layer features of the classifier (row indexed) and the dictionary atoms (column indexed). Each classifier feature is weakly aligned with multiple dictionary atoms.

<figure>
    <img src="/assets/img/shortcut/images/feature_alignment.png" width="350">
    <figcaption></figcaption>
</figure>

As seen below the first layer features are less parsimonious than the dictionary atoms shown above.

<figure>
    <img src="/assets/img/shortcut/images/first_layer_features.png" width="650">
    <figcaption></figcaption>
</figure>

We can probe further to understand why the above features are effective for classification. Our first step is to distill the model into a more interpretable form.

- For each sample in the training set, we decompose its configuration of 4 atomic features into a set of unigrams, bigrams, trigrams and 4-grams, where an n-gram is an order-agnostic combination of atomic features. Since the data is sampled from a limited set of configurations, the number of n-grams that decompose these configurations is manageable and is a lot smaller than the total number of possible n-grams.

- We initialize a counter for each (hidden-unit, n-gram) pair that keeps track of how often the hidden unit was activated when the n-gram was present in the input.

- We pass the samples through the classfier, and record how often each feature is activated when an n-gram is present in the input.

- Once we have passed every sample in the training set, we decompose each hidden unit into a logical expression of the form $$Out_j = \or (g_i)_{g_i \in S_j}$$ where $$g_i$$ denotes an n-gram, and $$S_j$$ denotes the set of n-grams that consistently activate unit $$j$$.

We can thus obtain a logical expression which is consistent with the neural network classifier and explains its prediction for a given sample from the dataset as follows:
