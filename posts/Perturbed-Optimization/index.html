<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="Perturbed Optimization" /><meta name="author" content="Elsa Riachi" /><meta property="og:locale" content="en" /><meta name="description" content="Many forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable with respect to their parameters either, their smooth approximations can be used to optimize over the underlying parameters." /><meta property="og:description" content="Many forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable with respect to their parameters either, their smooth approximations can be used to optimize over the underlying parameters." /><link rel="canonical" href="https://emr03.github.io/posts/Perturbed-Optimization/" /><meta property="og:url" content="https://emr03.github.io/posts/Perturbed-Optimization/" /><meta property="og:site_name" content="Elsa Riachi" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-18T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Perturbed Optimization" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Elsa Riachi" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Elsa Riachi"},"description":"Many forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable with respect to their parameters either, their smooth approximations can be used to optimize over the underlying parameters.","url":"https://emr03.github.io/posts/Perturbed-Optimization/","@type":"BlogPosting","headline":"Perturbed Optimization","dateModified":"2021-12-18T00:00:00+00:00","datePublished":"2021-12-18T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emr03.github.io/posts/Perturbed-Optimization/"},"@context":"https://schema.org"}</script><title>Perturbed Optimization | Elsa Riachi</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Elsa Riachi"><meta name="application-name" content="Elsa Riachi"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">Elsa Riachi</a></div><div class="site-subtitle font-italic">Notes and Research</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/Emr03" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['example','doamin.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Perturbed Optimization</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Perturbed Optimization</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Elsa Riachi </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Dec 18, 2021, 12:00 AM +0000" >Dec 18, 2021<i class="unloaded">2021-12-18T00:00:00+00:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1797 words">9 min read</span></div></div><div class="post-content"><div style="display:none"> Many forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable with respect to their parameters either, their smooth approximations can be used to optimize over the underlying parameters.</div><div style="display:none"> $$ \newcommand\testmacro[2]{\mathbf{F\alpha}(#1)^{#2}} \def\norm#1{\left\|{#1}\right\|} % A norm with 1 argument \newcommand\zeronorm[1]{\norm{#1}_0} % L0 norm \newcommand\onenorm[1]{\norm{#1}_1} % L1 norm \newcommand\twonorm[1]{\norm{#1}_2} % L2 norm \def\&lt;{\left\langle} % Angle brackets \def\&gt;{\right\rangle} \newcommand\inner[1]{\langle #1 \rangle} % inner product \newcommand\argmax{\mathop\mathrm{arg max}} % Defining math symbols \newcommand\argmin{\mathop\mathrm{arg min}} \newcommand{\pd}[2]{\frac{\partial{#1}}{\partial{#2}}} \newcommand{\pdd}[2]{\frac{\partial^2{#1}}{\partial{#2}^2}} $$</div><h3 id="introduction">Introduction</h3><p>Many forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable either, their smooth approximations can be used to optimize over the underlying parameters.</p><p>Consider a Linear Program of the form:</p>\[\begin{align} F(\theta) &amp;= \max _{y \in \mathcal{C}}\langle y, \theta\rangle \\ y^{*}(\theta) &amp;= \underset{y \in \mathcal{C}}{\arg \max }\langle y, \theta\rangle \end{align}\]<p>Where \(C\) is a convex polytope. The solution to the above LP is <em>almost</em> always uniquely determined by \(\theta\) and corresponds to a vertex of \(C\). To see why, consider the following inequality-form LP:</p>\[\begin{equation} \max _{y \in \mathcal{C}}\langle y, \theta\rangle,\ \text{ such that } Q y \geq b \end{equation}\]<p>for \(y \in \mathbb{R}^n\). And recall the following KKT condition, where \(\mathcal{A}\) denotes the set of active inequality constraints:</p>\[\begin{align} \theta &amp;= - Q^T \lambda^* \\ \lambda_i^* &amp;\neq 0 \ \ \forall i \in \mathcal{A} \end{align}\]<p>In general, \(n\) columns of \(Q^T\) are required to construct the n-dimnesional \(\theta\). So \(\lambda^*\) should have \(n\) non-zero entries. This means that \(y^*\) is the solution of a system of \(n\) linear equations \(\bar{Q}y = \bar{b}\), where \(\bar{Q}\) and \(\bar{b}\) denote a subset of the rows of \(Q\) and \(b\) respectively. Effectively, \(y^*\) is the intersections of \(n\) sides of the polytope, corresponding to a vertex point.</p><p>However, the maximizer \(y^{*}\) is not a differentiable function of \(\theta\). This is because, small perturbations in \(\theta\) do not change the active constraints at \(y^{*}\). While the entries \(\lambda_i\) might be different, the effective system of \(n\) linear equations remains the same, unless a large enough perturbation to \(\theta\) is applied. This means that the Jacobian of \(y^*\) with respect to \(\theta\) is zero for almost all \(\theta\), and undefined at the boundaries of normal cones. This is inconvenient when we wish to optimize the parameter \(\theta\) for some objective, as is often the case in non-differentiable learning or inverse problems.</p><figure> <img data-proofer-ignore data-src="/assets/img/optimization/normal_fan.png" width="650" /><figcaption>Figure 1: The normal fan of the polytope consists of the regions in which the cost parameters result in the same maximizing vertex.</figcaption></figure><p>Berthet et al. \cite{} propose to <em>smooth out</em> the above LP. The intuition behind this approach is as follows. Since we want \(y^{*}\) to vary smoothly in \(\mathcal{C}\) with respect to \(\theta\), we can consider an alternative solution which is a convex combination of the \(v\) vertices of \(\mathcal{C}\),</p>\[\begin{equation} y^{*}(\alpha) = \sum_{i=1}^{v} \alpha_i y_i \text{ where } \alpha_i \in [0, 1] \end{equation}\]<p>where \(\alpha \in \mathbb{R}^{v}\) is a function of \(\theta\) which will be revealed shortly. Intuitively, looking at the above figure, as \(\theta\) is moved clock-wise, we expect \(y^*(\alpha)\) to move closer to \(y_2\). In a way, we want all vertices to contribute to the resulting estimate, while keeping the estimate \(y^{*}(\alpha)\) close to the solution. Note that \(\alpha\) effectively defines a probability mass function \(p_{\theta}\) over the possible solutions \(\{y_i\}_{i=1}^{v}\).</p><p>The authors propose to achieve a smooth approximation to \(y^{*}(\theta)\) by taking the expectation of \(y^{*}(\theta + \epsilon Z)\) where \(\epsilon\) controls the size of the added noise, and \(Z\) is a random variable sampled from a Gibbs distribution \(\mu(Z) \propto e^{-\nu(Z)}\).</p>\[\begin{equation} y^{*}_\epsilon (\theta) = \mathbb{E}_z \mathrm{arg max}_{y \in C} \langle y,\theta + \epsilon Z \rangle \end{equation}\] \[\begin{equation} F_\epsilon (\theta) = \mathbb{E}_z \mathrm{max}_{y \in C} \langle y,\theta + \epsilon Z \rangle \end{equation}\]<p>The gradient of \(F(\theta) = \max_{y \in C}\langle y, \theta \rangle\) with respect to \(\theta\) is \(y^*\), as long as \(\theta\) remains in the normal cone where \(y^*\) is optimal. It is undefined on the boundaries of normal cones. There is a similar relationship between \(\nabla_\theta F_{\epsilon}(\theta)\) and \(y^*_{\epsilon}(\theta)\):</p>\[\begin{align} \nabla_{\theta}F_{\epsilon}(\theta) &amp;= \nabla_{\theta} \mathbb{E}_z \mathrm{max}_{y \in C} \langle y,\theta + \epsilon Z \rangle = \mathbb{E}_z \nabla_{\theta} \mathrm{max}_{y \in C} \langle y,\theta + \epsilon Z \rangle \\ \\ &amp;= \mathbb{E}_z \nabla_{\theta} \langle y^*(\theta + \epsilon Z), \theta \rangle = \mathbb{E}_z y^*(\theta + \epsilon Z) = y_{\epsilon}^*(\theta) \end{align}\]<p>This leads to the fact that:</p>\[\begin{equation} \nabla_{\theta} y_{\epsilon}^*(\theta) = \nabla_{\theta}^2 F_\epsilon(\theta) \end{equation}\]<h3 id="derivatives-of-soft-maximizers">Derivatives of Soft Maximizers</h3><p>To see why the smooth approximations are differentiable with respect to \(\theta\), consider the integral which evaluates \(F_\epsilon (\theta)\).</p>\[\begin{align} F_\epsilon (\theta) &amp;= \int_{Z} \max_{y \in C}\langle y, \theta + \epsilon z \rangle \mu(z) \, dz \\ \\ \nabla_{\theta} F_\epsilon (\theta) &amp;= \int_{Z} \nabla_{\theta} \max_{y \in C}\langle y, \theta + \epsilon z \rangle \mu(z) \, dz \\ \end{align}\]<p>Since the integral boundaries don’t depend on \(\theta\) all we have to do is differentiate the integrand and integrate the gradient. Since \(\max_{y \in C}\langle y, \theta + \epsilon z \rangle\) is not everywhere differentiable, we consider \(z' = \theta + \epsilon z\) to be a fixed parameter vector, so now \(z\) depends on \(z'\) and \(\theta\). A simple change of variable results in:</p>\[\begin{align} \nabla_{\theta} F_\epsilon (\theta) &amp;= \int_{Z'} \max_{y \in C}\langle y, z' \rangle \nabla_{\theta} \mu\left(\frac{z' - \theta}{\epsilon}\right) \, \frac{dz'}{\epsilon} \\ \\ &amp;= \int_{Z'} \max_{y \in C}\langle y, z' \rangle \mu\left(\frac{z' - \theta}{\epsilon}\right) \nabla_{\theta} \nu\left(\frac{z' - \theta}{\epsilon}\right) \, \frac{dz'}{\epsilon} \\ \\ &amp;= \mathbb{E} \left[ F(\theta + \epsilon Z) \nabla_{Z} \nu\left(Z \right) / \epsilon \right] \end{align}\]<p>It follows that:</p>\[\begin{align} \nabla_{\theta} y_{\epsilon}^{*}(\theta) &amp;= \nabla_{\theta}^{2} F_{\epsilon}(\theta) \\ \\ \nabla_{\theta} y_{\epsilon}^{*}(\theta) &amp;= \nabla_{\theta}^{2} \int_{Z} \max_{y \in C}\langle y, \theta + \epsilon z \rangle \mu(z) \, dz \\ \\ &amp;= - \int_{Z'} \max_{y \in C}\langle y, z' \rangle \nabla^{2}_{\theta} \mu\left(\frac{z' - \theta}{\epsilon}\right) \, \frac{dz'}{\epsilon} \\ \\ &amp;= - \int_{Z'} \max_{y \in C}\langle y, z' \rangle \nabla_{\theta} \left[ \mu\left(\frac{z' - \theta}{\epsilon}\right) \nabla_{\theta} \nu\left(\frac{z' - \theta}{\epsilon}\right) \right] \, \frac{dz'}{\epsilon} \\ \\ &amp;= \mathbf{E} \left[F(\theta+\varepsilon Z)\left(\nabla_{z} \nu(Z) \nabla_{z} \nu(Z)^{\top}-\nabla_{z}^{2} \nu(Z)\right) / \varepsilon^{2} \right] \end{align}\]<h3 id="monte-carlo-estimates-of-the-gradients">Monte-Carlo Estimates of the Gradients</h3><p>Using the following derived expression:</p>\[\begin{equation} \nabla_{\theta} y_{\epsilon}^{*}(\theta) = \mathbf{E} \left[F(\theta+\varepsilon Z)\left(\nabla_{z} \nu(Z) \nabla_{z} \nu(Z)^{\top}-\nabla_{z}^{2} \nu(Z)\right) / \varepsilon^{2} \right] \end{equation}\]<p>we see that we can estimate the derivative of \(\nabla_{\theta} y_{\epsilon}^{*}(\theta)\) by:</p><ul><li>sampling \(Z_i \sim \mu(Z)\) for \(i = 1, ... ,m\)<li>solving each LP \(\max_{y \in C} \langle y, \theta + \epsilon Z_i \rangle\)<li>computing \(\left(\nabla_{z} \nu(Z) \nabla_{z} \nu(Z)^{\top}-\nabla_{z}^{2} \nu(Z)\right) / \varepsilon^{2}\) at \(Z = Z_i\).</ul><h3 id="fenchel-young-losses-">Fenchel-Young Losses (??)</h3><p>First, we write \(F_\epsilon (\theta)\) with respect to \(\theta\) as a convex conjugate. Let \(\Omega(y)\) be the convex conjugate of \(F_1 (\theta)\). A simple change of variable, where \(\theta' := \epsilon \theta\) shows that the convex conjugate of \(F_{\epsilon}(\theta)\) is \(\epsilon \Omega(y)\).</p>\[\begin{align} \Omega(y) &amp;= \sup_{\theta}\{\langle \theta, y \rangle - F_1(\theta)\} \\ \\ \epsilon \Omega(y) &amp;= \sup_{\theta} \{\langle \epsilon \theta, y \rangle - \mathbb{E}_Z \mathrm{max}_{y \in C} \langle y, \epsilon \theta + \epsilon Z \rangle\} \\ \\ \epsilon \Omega(y) &amp;= \sup_{\theta'} \{\langle \theta', y \rangle - \mathbb{E}_Z \mathrm{max}_{y \in C} \langle y, \theta' + \epsilon Z \rangle\} \\ \\ \epsilon \Omega(y) &amp;= F^{*}_{\epsilon}(y) \end{align}\]<p>Then using the fact that \(F_{\epsilon}(\theta)\) is strictly convex:</p>\[\begin{align} F_{\epsilon}(\theta)= \sup_{y} \{\langle y, \theta \rangle - \epsilon \Omega(y)\} \end{align}\]<p>The supremum is achieved at \(y_{\epsilon}^{*}(\theta)\) where \(\epsilon \nabla_{y}\Omega(y_{\epsilon}^{*}) = \theta\). Using the property \(\nabla f^{*} = (\nabla f)^{-1}\),</p>\[\begin{align} \left( \epsilon \nabla_{y} \Omega(y_{\epsilon}^{*}) \right)^{-1} &amp;= \nabla_{\theta} F_{\epsilon}(\theta) \\ \\ y_{\epsilon}^{*}(\theta) &amp;= \nabla_{\theta} F_{\epsilon}(\theta) \end{align}\]<p>Consider the problem of maximizing the likelihood of observations \(\{y_i\}\), with respect to a parameterized model \(p_{\theta}(y)\), where \(p_{\theta}\) is a Gibbs distribution. The empirical loss is shown below.</p>\[\begin{align} \bar{\ell}_{n}(\theta) &amp;= \frac{1}{n} \sum_{i=1}^{n} \log p_{\text {Gibbs }, \theta}\left(y_{i}\right) \\ \\ &amp;= \frac{1}{n} \sum_{i=1}^{n}\left\langle y_{i}, \theta\right\rangle-\log Z(\theta)\\ \text { with } \nabla_{\theta} \bar{\ell}_{n}(\theta) &amp;= \frac{1}{n} \sum_{i=1}^{n} y_{i}-\mathbf{E}_{\text {Gibbs }, \theta}[Y] \end{align}\]<p>Notice that setting \(\nabla_{\theta} \bar{\ell}_{n}(\theta) = 0\) points to a moment matching procedure to fit \(\theta\). However,</p>\[\begin{align} F_{\epsilon}(\theta) &amp;= \mathbb{E}_Z \max_{y} \langle y, \theta + \epsilon Z \rangle \\ &amp;\geq \mathbb{E}_Z \log{ \mathbb{E}_Y exp\{\langle Y, \theta + \epsilon Z \rangle\} } \\ &amp;\geq Z_{\epsilon}(\theta) \\ \end{align}\]<p>So \(\left\langle y_{i}, \theta\right\rangle- F_{\epsilon}(\theta)\) is a lower bound on \(\ell}_{n}(\theta)\).</p><h3 id="how-good-are-mc-estimates-of-the-gradients">How Good are MC Estimates of the Gradients?</h3><p>To answer this question we need to develop asymptotic results about the MC estimates of \(\nabla_{\theta} y_{\epsilon}^{*}(\theta)\) and \(\nabla_{\theta} F_\epsilon (\theta)\). But before we do, since asymptotic results depend on properties of function of the random variable, we should first characterize \(F_\epsilon (\theta)\) and \(y_{\epsilon}^{*}(\theta)\).</p><p><strong>Properties of \(F_{\epsilon}\)</strong>:</p><ol><li>\(F_{\epsilon}\) is strictly convex (with respect to \(\theta\)).</ol><p><em>Proof</em>:</p><p>Since \(F\) is the supremum of a set of linear functions, it is convex. Let \(\lambda \in \[0, 1\]\), \(\theta_{\lambda}=\lambda \theta+(1-\lambda) \theta^{\prime}\). So:</p>\[\begin{equation} \lambda F_{\varepsilon}(\theta)+(1-\lambda) F_{\varepsilon}\left(\theta^{\prime}\right)=\mathbf{E}\left[\lambda F(\theta+\varepsilon Z)+(1-\lambda) F\left(\theta^{\prime}+\varepsilon Z\right)\right] \geqslant \mathbf{E}\left[F\left(\lambda \theta+(1-\lambda) \theta^{\prime}+\varepsilon Z\right)\right]=F_{\varepsilon}\left(\theta_{\lambda}\right) \end{equation}\]<p>To show that \(F_{\epsilon}\) is strictly convex, we need to show that the inequality above is strict for all values of \(\theta, \ \theta^{\prime} \in \mathbb{R}^{d}\). Note that the inequality applies to the arguments inside the expectation. Therefore, for the inequality to hold with equality, it should hold for <em>almost</em> all values of \(Z\).</p><p>If \(\lambda F(\theta+\varepsilon z)+(1-\lambda) F\left(\theta^{\prime}+\varepsilon z\right) \geqslant F\left(\lambda \theta+(1-\lambda) \theta^{\prime}+\varepsilon z\right)\), for almost all \(z\), then \(F\) is linear over the segment \([\theta + \epsilon z, \theta^{\prime} + \epsilon z]\) for almost all \(z \in \mathbb{R}^{d}\). Since \(F\) is the result of a linear program, \(F\) is the value obtained by \(\langle \theta + \epsilon z, y^{*} \rangle\) at \(y^{*}\) a vertex point in \(\mathcal{C}\). So if \(F\) is linear in \([\theta, \theta^{\prime}]\) then \(y^{*}(\theta + \epsilon z)\) remains the same as \(\theta\) is varied on the line \([\theta, \theta^{\prime}]\), for almost all \(z\). For this to hold, \(\theta + \epsilon z\) and \(\theta^{\prime} + \epsilon z\) need to be in the same normal cone of \(\mathcal{C}\) for almost all \(z\). This can only happen if \(\theta - \theta^{\prime} = 0\). So \(F_{\epsilon}\) is strictly convex.</p><ol><li>\(F_{\epsilon}\) is \(R_C-Lipschitz\).</ol><p><em>Proof</em>: Since \(F_{\epsilon}\) is the supremum of a set of linear functions, and \(\twonorm{y} \leq R_C\), then \(| F_{epsilon}(\theta) - F_{\epsilon}(\theta^{\prime}) | \leq R_C (\theta - \theta^{\prime})\).</p><ol><li>\(F_{\epsilon}\) is \(R_C M_{\mu} / \epsilon\)-gradient Lipschitz.</ol><p><em>Proof</em>:</p><p>Recall that \(\nabla_{\theta} F_\epsilon (\theta) = \mathbb{E} F(\theta + \epsilon Z) \frac{\nabla_{z}\mu(z)}{\epsilon}\). Then,</p>\[\begin{align} \twonorm{F_\epsilon (\theta) - F_\epsilon (\theta^{\prime})} &amp;= \twonorm{ \mathbb{E} \left[ F(\theta + \epsilon Z) - F(\theta^{\prime} + \epsilon Z) \frac{\nabla_{z}\mu(z)}{\epsilon} \right]}\\ \end{align}\]<p>Using Jensen’s inequality since the $l2$ norm is convex, and the Cauchy-Schwarz inequality we obtain:</p>\[\begin{align} &amp; \leq \mathbb{E} \twonorm{ F(\theta + \epsilon Z) - F(\theta^{\prime})} \mathbb{E} \twonorm{\frac{\nabla_{z}\mu(z)}{\epsilon}} \\ &amp; \leq R_C (\theta - \theta^{\prime}) M_{\mu} / {\epsilon} \\ \end{align}\]<p>**Asymptotic Convergence of $$\</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/notes/'>Notes</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/optimization/" class="post-tag no-text-decoration" >optimization</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Perturbed Optimization - Elsa Riachi&url=https://emr03.github.io/posts/Perturbed-Optimization/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Perturbed Optimization - Elsa Riachi&u=https://emr03.github.io/posts/Perturbed-Optimization/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Perturbed Optimization - Elsa Riachi&url=https://emr03.github.io/posts/Perturbed-Optimization/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Constrained-Optimization-the-KKT-conditions/">Constrained Optimization, The KKT conditions</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/optimization/">optimization</a> <a class="post-tag" href="/tags/adversarial-robustness/">adversarial robustness</a> <a class="post-tag" href="/tags/graphs/">graphs</a> <a class="post-tag" href="/tags/sparsity/">sparsity</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Constrained-Optimization-the-KKT-conditions/"><div class="card-body"> <span class="timeago small" >Jul 24, 2021<i class="unloaded">2021-07-24T00:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Constrained Optimization, The KKT conditions</h3><div class="text-muted small"><p> In this post I present an intuitive description of constrained optimization. It is by no means self-contained. A bit of background knowledge in constrained optimization is assumed. This post is mo...</p></div></div></a></div><div class="card"> <a href="/posts/Duality-I/"><div class="card-body"> <span class="timeago small" >Nov 8, 2021<i class="unloaded">2021-11-08T00:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Duality I</h3><div class="text-muted small"><p> In this post I present a brief overview of duality in optimization. Introduction Consider a constrained optimization problem: [\begin{equation} \min {x \in \mathbb{R}^{n}} f(x) \quad \text { ...</p></div></div></a></div><div class="card"> <a href="/posts/Spectral-Clustering/"><div class="card-body"> <span class="timeago small" >Oct 31, 2021<i class="unloaded">2021-10-31T00:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Spectral Clustering from the Min Cut Problem</h3><div class="text-muted small"><p> Spectral clustering, often used as a community detection method, has its origins in the min-cut problem in graph theory. While the min-cut problem is NP-hard in the general case, a relaxation of t...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Iterated-Soft-Thresholding/" class="btn btn-outline-primary" prompt="Older"><p>Iterated Soft Thresholding</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/username">Elsa Riachi</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/optimization/">optimization</a> <a class="post-tag" href="/tags/adversarial-robustness/">adversarial robustness</a> <a class="post-tag" href="/tags/graphs/">graphs</a> <a class="post-tag" href="/tags/sparsity/">sparsity</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
