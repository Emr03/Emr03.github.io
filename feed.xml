<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://emr03.github.io/</id><title>Elsa Riachi</title><subtitle>A page for personal projects and notes.</subtitle> <updated>2022-01-03T05:26:28+00:00</updated> <author> <name>Elsa Riachi</name> <uri>https://emr03.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://emr03.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://emr03.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator> <rights> Â© 2022 Elsa Riachi </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Perturbed Optimization</title><link href="https://emr03.github.io/posts/Perturbed-Optimization/" rel="alternate" type="text/html" title="Perturbed Optimization" /><published>2021-12-18T00:00:00+00:00</published> <updated>2021-12-18T00:00:00+00:00</updated> <id>https://emr03.github.io/posts/Perturbed-Optimization/</id> <content src="https://emr03.github.io/posts/Perturbed-Optimization/" /> <author> <name>Elsa Riachi</name> </author> <category term="Notes" /> <summary> Many forward computations whose parameters we would like to optimize may include discrete, or non-differentiable elements. Often, these forward procedures can be expressed as or approximated by the solution of a linear program. While linear programs are not differentiable with respect to their parameters either, their smooth approximations can be used to optimize over the underlying parameters... </summary> </entry> <entry><title>Iterated Soft Thresholding</title><link href="https://emr03.github.io/posts/Iterated-Soft-Thresholding/" rel="alternate" type="text/html" title="Iterated Soft Thresholding" /><published>2021-11-08T00:00:00+00:00</published> <updated>2021-11-08T00:00:00+00:00</updated> <id>https://emr03.github.io/posts/Iterated-Soft-Thresholding/</id> <content src="https://emr03.github.io/posts/Iterated-Soft-Thresholding/" /> <author> <name>Elsa Riachi</name> </author> <category term="Notes" /> <summary> $$ \newcommand\testmacro[2]{\mathbf{F\alpha}(#1)^{#2}} \def\norm#1{\left\|{#1}\right\|} % A norm with 1 argument \newcommand\zeronorm[1]{\norm{#1}_0} % L0 norm \newcommand\onenorm[1]{\norm{#1}_1} % L1 norm \newcommand\twonorm[1]{\norm{#1}_2} % L2 norm \def\&amp;lt;{\left\langle} % Angle brackets \def\&amp;gt;{\right\rangle} \newcommand\inner[1]{\langle #1 \rangle} % inner product \newcommand\argmax{\m... </summary> </entry> <entry><title>Duality I</title><link href="https://emr03.github.io/posts/Duality-I/" rel="alternate" type="text/html" title="Duality I" /><published>2021-11-08T00:00:00+00:00</published> <updated>2021-11-08T00:00:00+00:00</updated> <id>https://emr03.github.io/posts/Duality-I/</id> <content src="https://emr03.github.io/posts/Duality-I/" /> <author> <name>Elsa Riachi</name> </author> <category term="Notes" /> <summary> In this post I present a brief overview of duality in optimization. Introduction Consider a constrained optimization problem: \[\begin{equation} \min _{x \in \mathbb{R}^{n}} f(x) \quad \text { subject to }\left\{\begin{array}{ll} c_{i}(x)=0, &amp;amp; i \in \mathcal{E} \\ c_{i}(x) \geq 0, &amp;amp; i \in \mathcal{I}, \end{array}\right. \end{equation}\] In a previous post on the KKT conditions, w... </summary> </entry> <entry><title>Spectral Clustering from the Min Cut Problem</title><link href="https://emr03.github.io/posts/Spectral-Clustering/" rel="alternate" type="text/html" title="Spectral Clustering from the Min Cut Problem" /><published>2021-10-31T00:00:00+00:00</published> <updated>2021-10-31T00:00:00+00:00</updated> <id>https://emr03.github.io/posts/Spectral-Clustering/</id> <content src="https://emr03.github.io/posts/Spectral-Clustering/" /> <author> <name>Elsa Riachi</name> </author> <category term="Notes" /> <summary> Spectral clustering, often used as a community detection method, has its origins in the min-cut problem in graph theory. While the min-cut problem is NP-hard in the general case, a relaxation of the problem can be obtained from the graph Laplacian. $$ \newcommand\testmacro[2]{\mathbf{F\alpha}(#1)^{#2}} \def\norm#1{\left\|{#1}\right\|} % A norm with 1 argument \newcommand\zeronorm[1]{\norm{#... </summary> </entry> <entry><title>How Adversarial Attacks Exploit Shortcuts</title><link href="https://emr03.github.io/posts/How-Adversarial-Attacks-Exploit-Shortcuts/" rel="alternate" type="text/html" title="How Adversarial Attacks Exploit Shortcuts" /><published>2021-08-16T00:00:00+00:00</published> <updated>2021-08-16T00:00:00+00:00</updated> <id>https://emr03.github.io/posts/How-Adversarial-Attacks-Exploit-Shortcuts/</id> <content src="https://emr03.github.io/posts/How-Adversarial-Attacks-Exploit-Shortcuts/" /> <author> <name>Elsa Riachi</name> </author> <category term="Research" /> <summary> Recent investigations into the nature of adversarial attacks have emphasized the presence of non-robust features in the dataset. Understanding the nature of non-robust features has proven difficult since it requires understanding the set of uninterpretable pixel patterns which neural networks learn to associate with the class label. However, despite this difficulty, a deeper insight into the na... </summary> </entry> </feed>
